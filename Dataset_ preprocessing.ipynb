{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying two datasets into one\n",
    "\n",
    "- Football-Player-Detection (v8-resized1280_tile2x2_aug3x.yolov5pytorch)\n",
    "    - Classes: football, player (one image marked null)\n",
    "    - Resized to 1280x1280 (Stretch)\n",
    "    - Normalised bb\n",
    "- SOD_Dataset\n",
    "    - Classes: ball, player\n",
    "    - Annotations: For each image, there is a text file containing the class ID, Xmin, Ymin, Xmax, and Ymax, respectively.\n",
    "    - yolov5_annotations: Box coordinates are in normalized xywh format (from 0 - 1) for yolov5_annotations. x_center and width are divided by image width, and y_center and height by image height.\n",
    "\n",
    " \n",
    " # Final dataset\n",
    " - Classes: football, player\n",
    " - Resized to 1280x1280 (Stretch)\n",
    " - Annotations: For each image, there is a text file containing the class ID, Xmin, Ymin, Xmax, and Ymax, respectively. (bottom left, top right corners ?Â¿)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Important Note\n",
    "The two oringinal datasets must be downloaded into a \"Datasets\" folder in the parent folder of the repository:\n",
    "\n",
    "Download from:\n",
    "- https://github.com/FootballAnalysis/footballanalysis/tree/main/Dataset/Object%20Detection%20Dataset\n",
    "- https://universe.roboflow.com/augmented-startups/football-player-detection-kucab\n",
    "\n",
    "And name, respectively:\n",
    "- SOD_Dataset\n",
    "- Football-Player-Detection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping SOD_Dataset and switching labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set the path to your image directory\n",
    "image_dir = \"Datasets/SOD_Dataset/images\"\n",
    "image_output_dir = \"Datasets/SOD_Dataset/resized_images\"\n",
    "os.makedirs(image_output_dir, exist_ok = True)\n",
    "\n",
    "# Set the path to your label directory\n",
    "label_dir = \"Datasets/SOD_Dataset/annotations\"\n",
    "label_output_dir = \"Datasets/SOD_Dataset/resized_annotations\"\n",
    "os.makedirs(label_output_dir, exist_ok = True)\n",
    "\n",
    "new_size = (1280, 1280)\n",
    "\n",
    "# Loop through each image file in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Open the image file using PIL\n",
    "    img = Image.open(os.path.join(image_dir, filename))\n",
    "    \n",
    "    # Get the size of the image\n",
    "    # width, height = img.size\n",
    "    # Print the image size\n",
    "    # print(\"Image size for {}: {} x {}\".format(filename, width, height))\n",
    "\n",
    "    # Resize image\n",
    "    new_image = img.resize(new_size)\n",
    "    new_image.save(os.path.join(image_output_dir, filename))\n",
    "\n",
    "    # Open the corresponding label file\n",
    "    label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    label_path = os.path.join(label_dir, label_filename)\n",
    "    with open(label_path, \"r\") as f:\n",
    "        label_data = f.readlines()\n",
    "     # Resize the bounding boxes in the label file\n",
    "    new_label_data = []\n",
    "    for line in label_data:\n",
    "        parts = line.strip().split(\" \")\n",
    "        class_id = parts[0]\n",
    "        bounding_box = parts[1:]\n",
    "\n",
    "        x_min = float(bounding_box[0])\n",
    "        y_min = float(bounding_box[1])\n",
    "        x_max = float(bounding_box[2])\n",
    "        y_max = float(bounding_box[3])\n",
    "        \n",
    "        # Resize the bounding box coordinates\n",
    "        x_min_resized = int(x_min * (new_size[0] / img.width))\n",
    "        y_min_resized = int(y_min * (new_size[1] / img.height))\n",
    "        x_max_resized = int(x_max * (new_size[0] / img.width))\n",
    "        y_max_resized = int(y_max * (new_size[1] / img.height))\n",
    "        \n",
    "        # Change class id to match other dataset:\n",
    "        new_class_id = class_id # in case there is any null \n",
    "        if class_id == '0':\n",
    "            new_class_id = '1'\n",
    "        elif class_id == '1': \n",
    "            new_class_id = '0'\n",
    "        \n",
    "        new_parts = [new_class_id, str(x_min_resized), str(y_min_resized), str(x_max_resized), str(y_max_resized)]\n",
    "        new_label_data.append(\" \".join(new_parts))\n",
    "    \n",
    "        output_label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        output_label_path = os.path.join(label_output_dir, output_label_filename)\n",
    "        with open(output_label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(new_label_data))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Bounding boxes in  Football-Player-Detection to correct format and joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the path to your image directory\n",
    "image_directories = [\"Datasets/Football-Player-Detection/train/images\", \"Datasets/Football-Player-Detection/test/images\", \"Datasets/Football-Player-Detection/valid/images\"]\n",
    "image_output_dir = \"Datasets/Football-Player-Detection/resized_images\"\n",
    "os.makedirs(image_output_dir, exist_ok = True)\n",
    "# Set the path to your label directory\n",
    "label_dirrectories = [\"Datasets/Football-Player-Detection/train/labels\", \"Datasets/Football-Player-Detection/test/labels\", \"Datasets/Football-Player-Detection/valid/labels\"]\n",
    "label_output_dir = \"Datasets/Football-Player-Detection/resized_annotations\"\n",
    "os.makedirs(label_output_dir, exist_ok = True)\n",
    "\n",
    "\n",
    "for image_dir, label_dir in zip(image_directories, label_dirrectories) :\n",
    "    # Loop through each image file in the directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        #  save in new folder MAYBE LATER ALSO RESIZE; BUT THIS IS FASTER NOW\n",
    "        #img = Image.open(os.path.join(image_dir, filename))\n",
    "        #img.save(os.path.join(image_output_dir, filename))\n",
    "        original_im_dir = os.path.join(image_dir, filename)\n",
    "        target_im_dir = os.path.join(image_output_dir, filename)\n",
    "\n",
    "        shutil.move(original_im_dir, target_im_dir)\n",
    "\n",
    "        # Open the corresponding label file\n",
    "        label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        label_path = os.path.join(label_dir, label_filename)\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = f.readlines()\n",
    "        # Resize the bounding boxes in the label file\n",
    "            # Resize the bounding boxes in the label file\n",
    "        new_label_data = []\n",
    "        for line in label_data:\n",
    "            parts = line.strip().split(\" \")\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            box_width = float(parts[3])\n",
    "            box_height = float(parts[4])\n",
    "            \n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            x_min = int((x_center - (box_width / 2)) * img.width)\n",
    "            y_min = int((y_center - (box_height / 2)) * img.height)\n",
    "            x_max = int((x_center + (box_width / 2)) * img.width)\n",
    "            y_max = int((y_center + (box_height / 2)) * img.height)\n",
    "            \n",
    "            new_parts = [parts[0], str(x_min), str(y_min), str(x_max), str(y_max)]\n",
    "            new_label_data.append(\" \".join(new_parts))\n",
    "        \n",
    "            output_label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_label_path = os.path.join(label_output_dir, output_label_filename)\n",
    "            with open(output_label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(new_label_data))\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dan's Framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# this function is for the DataLoader, it makes sure the tensors within a batch are the same dimension (don't ask how, I don't know)\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# this is a class that loads the data, according to how pytorch wants it\n",
    "class playersDataset(Dataset): # dataset is the path\n",
    "    def __init__(self, folder_path, img_size=416):\n",
    "        self.root = folder_path\n",
    "        self.images_folder = os.path.join(folder_path, \"resized_images\")\n",
    "        self.labels_folder = os.path.join(folder_path, \"resized_annotations\")\n",
    "        \n",
    "        self.image_files = os.listdir(self.images_folder)\n",
    "        self.label_files = os.listdir(self.labels_folder)\n",
    "\n",
    "        self.convert_tensor = transforms.ToTensor()\n",
    "\n",
    "    def readLabelsFile(self, file_path, index):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    " \n",
    "        with open(file_path) as f:\n",
    "            for row in f:\n",
    "                annotation = [float(x) for x in row.split()]\n",
    "                #print(annotation)\n",
    "                labels.append(int(annotation[0]))\n",
    "                [x0, y0, x1, y1] = annotation[1:5]\n",
    "                boxes.append([x0, y0, x1, y1])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)         \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\" : boxes,\n",
    "            \"labels\" : labels,\n",
    "            \"image_id\" : torch.tensor(index),\n",
    "            \"area\" : areas,\n",
    "            \"iscrowd\" : iscrowd\n",
    "            }\n",
    "\n",
    "        return target\n",
    "\n",
    "\n",
    "    # pytorch needs this, it returns a single (image, output) pair\n",
    "    def __getitem__(self, index):\n",
    "        # load and format the image file as a tensor\n",
    "        \n",
    "        imgPath = os.path.join(self.images_folder, self.image_files[index])\n",
    "        img = Image.open(imgPath)\n",
    "\n",
    "        # TEMPORARY - REMOVE THIS LATER, WE SHOULD DECIDE HOW LARGE THE IMAGES ARE\n",
    "        img = img.resize((1080, 1920))\n",
    "\n",
    "        input_img = self.convert_tensor(img)\n",
    "\n",
    "        # load and format the corresponding labels\n",
    "        labelPath = os.path.join(self.labels_folder, self.label_files[index])\n",
    "        target = self.readLabelsFile(labelPath, index)\n",
    "        \n",
    "        return input_img, target\n",
    "\n",
    "    # pytorch also needs the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "dataset1_dir =  \"Datasets/SOD_Dataset\"\n",
    "dataset2_dir = \"Datasets/Football-Player-Detection\"\n",
    "\n",
    "dataset1 = playersDataset(dataset1_dir)\n",
    "dataset2 = playersDataset(dataset2_dir)\n",
    "# Unifying datasets\n",
    "dataset = ConcatDataset([dataset1, dataset2])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the new DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2a\\Deep Learning\\assignment 2\\DeepLearning-2023\\Dataset_ preprocessing.ipynb Cell 12\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(new_dataset_dir, exist_ok \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Loop over each image and its corresponding label file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (img, boxes, labels, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Save image to new directory\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     img_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(new_dataset_dir, img_filename)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:243\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2a\\Deep Learning\\assignment 2\\DeepLearning-2023\\Dataset_ preprocessing.ipynb Cell 12\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# TEMPORARY - REMOVE THIS LATER, WE SHOULD DECIDE HOW LARGE THE IMAGES ARE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mresize((\u001b[39m1080\u001b[39m, \u001b[39m1920\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m input_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tensor(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# load and format the corresponding labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/DeepLearning-2023/Dataset_%20preprocessing.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m labelPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_folder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_files[index])\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Create new directory for concatenated dataset\n",
    "new_dataset_dir = \"Datasets/Concatenated_Dataset\"\n",
    "os.makedirs(new_dataset_dir, exist_ok = True)\n",
    "\n",
    "# Loop over each image and its corresponding label file\n",
    "for i, (img, boxes, labels, _) in enumerate(dataset):\n",
    "    # Save image to new directory\n",
    "    img_filename = f\"img_{i}.jpg\"\n",
    "    img_path = os.path.join(new_dataset_dir, img_filename)\n",
    "    img.save(img_path)\n",
    "    \n",
    "    # Save label file to new directory\n",
    "    label_filename = f\"img_{i}.txt\"\n",
    "    label_path = os.path.join(new_dataset_dir, label_filename)\n",
    "    \n",
    "    # Rescale bounding boxes to match new image size\n",
    "    boxes[:, [0, 2]] *= img.width\n",
    "    boxes[:, [1, 3]] *= img.height\n",
    "    \n",
    "    # Save label file with rescaled bounding boxes\n",
    "    with open(label_path, \"w\") as f:\n",
    "        for box, label in zip(boxes, labels):\n",
    "            f.write(f\"{label} {box[0]:.4f} {box[1]:.4f} {box[2]:.4f} {box[3]:.4f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Define the size of each set\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Define the random samplers for each set\n",
    "train_sampler = SubsetRandomSampler(range(train_size))\n",
    "val_sampler = SubsetRandomSampler(range(train_size, train_size + val_size))\n",
    "test_sampler = SubsetRandomSampler(range(train_size + val_size, len(dataset)))\n",
    "\n",
    "# Define the dataloaders for each set\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to visualize an image with the bounding boxes \n",
    "(error NumPy is not available, idk whats going on yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2a\\Deep Learning\\assignment 2\\Dataset_ preprocessing.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Get a random batch of images and their corresponding bounding boxes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(img,dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64))\u001b[39m/\u001b[39m\u001b[39m255.0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m images, boxes, labels, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Convert the image and bounding boxes to PIL format\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image \u001b[39m=\u001b[39m to_pil(images[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:243\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2a\\Deep Learning\\assignment 2\\Dataset_ preprocessing.ipynb Cell 11\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# TEMPORARY - REMOVE THIS LATER, WE SHOULD DECIDE HOW LARGE THE IMAGES ARE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mresize((\u001b[39m1080\u001b[39m, \u001b[39m1920\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m input_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tensor(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# load and format the corresponding labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202a/Deep%20Learning/assignment%202/Dataset_%20preprocessing.ipynb#X31sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m labelPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_folder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_files[index])\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transform to convert the tensor back to a PIL Image\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# Get a random batch of images and their corresponding bounding boxes\n",
    "torch.tensor(np.array(img,dtype=np.float64))/255.0\n",
    "images, boxes, labels, _ = next(iter(train_loader))\n",
    "\n",
    "# Convert the image and bounding boxes to PIL format\n",
    "image = to_pil(images[0])\n",
    "boxes = boxes[0]\n",
    "\n",
    "# Plot the image with the bounding boxes\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "# Loop over each bounding box and draw it on the image\n",
    "for box in boxes:\n",
    "    # Convert the normalized coordinates to pixel coordinates\n",
    "    x, y, w, h = box.tolist()\n",
    "    x1 = x - w/2\n",
    "    y1 = y - h/2\n",
    "    x2 = x + w/2\n",
    "    y2 = y + h/2\n",
    "    \n",
    "    # Draw the bounding box on the image\n",
    "    rect = plt.Rectangle((x1, y1), w, h, fill=False, edgecolor='r', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Show the image\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, ann_folder):\n",
    "        self.img_folder = img_folder\n",
    "        self.ann_folder = ann_folder\n",
    "        self.img_filenames = os.listdir(img_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image\n",
    "        img_filename = self.img_filenames[idx]\n",
    "        img_path = os.path.join(self.img_folder, img_filename)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Read annotation\n",
    "        ann_filename = os.path.splitext(img_filename)[0] + '.txt'\n",
    "        ann_path = os.path.join(self.ann_folder, ann_filename)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(ann_path, 'r') as f:\n",
    "            for line in f:\n",
    "                box = list(map(float, line.strip().split()))\n",
    "                labels.append(int(box[0]))\n",
    "                boxes.append(box[1:])\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "        boxes = torch.tensor(boxes)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return img, boxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
