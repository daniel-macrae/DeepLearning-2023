{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying two datasets into one\n",
    "\n",
    "- Football-Player-Detection\n",
    "    - Classes: football, player (one image marked null)\n",
    "    - Normalised bb\n",
    "- SOD_Dataset\n",
    "    - Classes: ball, player\n",
    "    - Annotations: For each image, there is a text file containing the class ID, Xmin, Ymin, Xmax, and Ymax, respectively.\n",
    "    - yolov5_annotations: Box coordinates are in normalized xywh format (from 0 - 1) for yolov5_annotations. x_center and width are divided by image width, and y_center and height by image height.\n",
    "\n",
    " \n",
    " # Final dataset\n",
    " - Classes: football, player\n",
    " - Resized to 1280 x 720 (Stretch)\n",
    " - Annotations: For each image, there is a text file containing the class ID, Xmin, Ymin, Xmax, and Ymax, respectively. (bottom left, top right corners ?Â¿)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Important Note\n",
    "The two oringinal datasets must be downloaded into a \"Datasets\" folder in the parent folder of the repository:\n",
    "\n",
    "Download from:\n",
    "- https://github.com/FootballAnalysis/footballanalysis/tree/main/Dataset/Object%20Detection%20Dataset\n",
    "- https://universe.roboflow.com/augmented-startups/football-player-detection-kucab\n",
    "\n",
    "And named, respectively:\n",
    "- SOD_Dataset\n",
    "- Football-Player-Detection\n",
    "***\n",
    "\n",
    "### There is a file with an empty annotations folder, it needs to be removed from the \"football-player-detection\" dataset, I have added an exception so that it doesn't read the files: \n",
    "- 536_pp_jpg.rf.533a9e3c21d059812e90fe2547ab60a4.jpg  \n",
    "- 536_pp_jpg.rf.533a9e3c21d059812e90fe2547ab60a4.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Create new directory for concatenated dataset\n",
    "new_dataset_dir = \"Datasets/Concatenated_Dataset/\"\n",
    "os.makedirs(new_dataset_dir, exist_ok = True)\n",
    "\n",
    "image_final_dir = \"Datasets/Concatenated_Dataset/images/\"\n",
    "os.makedirs(image_final_dir, exist_ok = True)\n",
    "\n",
    "label_final_dir = \"Datasets/Concatenated_Dataset/annotations/\"\n",
    "os.makedirs(label_final_dir, exist_ok = True)\n",
    "\n",
    "new_file_name = \"Sample_\"\n",
    "sample_n = 0\n",
    "\n",
    "\n",
    "#new_size = (1280, 1280)\n",
    "new_size = (320, 320)\n",
    "resize = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBB(x_min, y_min, x_max, y_max):\n",
    "    # prevent flipping, of the BB coordinates, pytorch gets angry otherwise\n",
    "    if x_max < x_min:\n",
    "        temp = x_min\n",
    "        x_min = x_max\n",
    "        x_max = temp\n",
    "    if y_max < y_min:\n",
    "        temp = y_min\n",
    "        y_min = y_max\n",
    "        y_max = temp\n",
    "\n",
    "    # stop 0 width and 0 height BBs\n",
    "    if abs(x_max - x_min) < 4:\n",
    "        x_min -= 5\n",
    "        x_max += 5\n",
    "    if abs(y_max - y_min) < 4:\n",
    "        y_min -= 5\n",
    "        y_max += 5\n",
    "\n",
    "    return x_min, y_min, x_max, y_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping SOD_Dataset and switching labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import numpy\n",
    "\n",
    "# Set the path to your image directory\n",
    "image_dir = \"Datasets/SOD_Dataset/images\"\n",
    "# Set the path to your label directory\n",
    "label_dir = \"Datasets/SOD_Dataset/annotations\"\n",
    "\n",
    "size1 = (0,0,3)\n",
    "size2 = (10000, 10000,3)\n",
    "\n",
    "# Loop through each image file in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Open the image file using PIL\n",
    "\n",
    "    original_im_dir = os.path.join(image_dir, filename)\n",
    "    target_im_dir = os.path.join(image_final_dir, new_file_name + str(sample_n) + \".jpg\")\n",
    "    \n",
    "    \n",
    "    img = Image.open(original_im_dir)\n",
    "\n",
    "    if np.shape(img) < size2:\n",
    "        size2 = np.shape(img)\n",
    "\n",
    "    if np.shape(img) > size1:\n",
    "        size1 = np.shape(img)\n",
    "\n",
    "    # Resize image\n",
    "    if resize:\n",
    "        new_image = img.resize(new_size)\n",
    "        new_image.save(target_im_dir)\n",
    "    else:\n",
    "        shutil.copy(original_im_dir, target_im_dir)\n",
    "\n",
    "    # Open the corresponding label file\n",
    "    label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    label_path = os.path.join(label_dir, label_filename)\n",
    "    with open(label_path, \"r\") as f:\n",
    "        label_data = f.readlines()\n",
    "     # Resize the bounding boxes in the label file\n",
    "    new_label_data = []\n",
    "    for line in label_data:\n",
    "        parts = line.strip().split(\" \")\n",
    "        class_id = parts[0]\n",
    "        bounding_box = parts[1:]\n",
    "\n",
    "        x_min = float(bounding_box[0])\n",
    "        y_min = float(bounding_box[1])\n",
    "        x_max = float(bounding_box[2])\n",
    "        y_max = float(bounding_box[3])\n",
    "        \n",
    "        if resize:\n",
    "            # Resize the bounding box coordinates\n",
    "            x_min_resized = int(x_min * (new_size[0] / img.width))\n",
    "            y_min_resized = int(y_min * (new_size[1] / img.height))\n",
    "            x_max_resized = int(x_max * (new_size[0] / img.width))\n",
    "            y_max_resized = int(y_max * (new_size[1] / img.height))\n",
    "        else:\n",
    "            # Resize the bounding box coordinates\n",
    "            x_min_resized = int(x_min * img.width)\n",
    "            y_min_resized = int(y_min * img.height)\n",
    "            x_max_resized = int(x_max * img.width)\n",
    "            y_max_resized = int(y_max * img.height)\n",
    "\n",
    "        x_min_resized, y_min_resized, x_max_resized, y_max_resized = checkBB(x_min_resized, y_min_resized, x_max_resized, y_max_resized)\n",
    "            \n",
    "        \n",
    "        # Change class id to match other dataset:\n",
    "        new_class_id = class_id # in case there is any null \n",
    "        if class_id == '0':\n",
    "            new_class_id = '1'\n",
    "        elif class_id == '1': \n",
    "            new_class_id = '0'\n",
    "        \n",
    "        new_parts = [new_class_id, str(x_min_resized), str(y_min_resized), str(x_max_resized), str(y_max_resized)]\n",
    "        new_label_data.append(\" \".join(new_parts))\n",
    "    \n",
    "        output_label_filename = os.path.splitext(new_file_name + str(sample_n))[0] + \".txt\"\n",
    "        output_label_path = os.path.join(label_final_dir, output_label_filename)\n",
    "    with open(output_label_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(new_label_data))\n",
    "    \n",
    "    sample_n += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Bounding boxes in  Football-Player-Detection to correct format and joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped erroneous file\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the path to your image directory\n",
    "image_directories = [\"Datasets/Football-Player-Detection/train/images\", \"Datasets/Football-Player-Detection/test/images\", \"Datasets/Football-Player-Detection/valid/images\"]\n",
    "\n",
    "# Set the path to your label directory\n",
    "label_directories = [\"Datasets/Football-Player-Detection/train/labels\", \"Datasets/Football-Player-Detection/test/labels\", \"Datasets/Football-Player-Detection/valid/labels\"]\n",
    "\n",
    "# RESIZING IS DEFINED IN THE PREVIOUS CODE\n",
    "\n",
    "for image_dir, label_dir in zip(image_directories, label_directories):\n",
    "    # Loop through each image file in the directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "\n",
    "        if filename == \"536_pp_jpg.rf.533a9e3c21d059812e90fe2547ab60a4.jpg\":\n",
    "            print(\"skipped erroneous file\")\n",
    "            continue # skips the rest of this iteration of the loop\n",
    "        else:\n",
    "        \n",
    "            original_im_dir = os.path.join(image_dir, filename)\n",
    "            target_im_dir = os.path.join(image_final_dir, new_file_name + str(sample_n) + \".jpg\")\n",
    "\n",
    "            img = Image.open(original_im_dir)\n",
    "            #print(np.shape(img))\n",
    "\n",
    "            if resize:\n",
    "                new_image = img.resize(new_size)\n",
    "                new_image.save(os.path.join(image_final_dir, new_file_name + str(sample_n) + \".jpg\"))\n",
    "            else:\n",
    "                shutil.copy(original_im_dir, target_im_dir) # just copy over the file\n",
    "\n",
    "\n",
    "            # Open the corresponding label file\n",
    "            label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            label_path = os.path.join(label_dir, label_filename)\n",
    "\n",
    "            with open(label_path, \"r\") as f:\n",
    "                label_data = f.readlines()\n",
    "            # Resize the bounding boxes in the label file\n",
    "                # Resize the bounding boxes in the label file\n",
    "            new_label_data = []\n",
    "            for line in label_data:\n",
    "                parts = line.strip().split(\" \")\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                box_width = float(parts[3])\n",
    "                box_height = float(parts[4])\n",
    "                \n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                x_min = int((x_center - (box_width / 2)) * img.width)\n",
    "                y_min = int((y_center - (box_height / 2)) * img.height)\n",
    "                x_max = int((x_center + (box_width / 2)) * img.width)\n",
    "                y_max = int((y_center + (box_height / 2)) * img.height)\n",
    "\n",
    "                if resize:\n",
    "                    x_min = int(x_min * (new_size[0] / img.width))\n",
    "                    y_min = int(y_min * (new_size[1] / img.height))\n",
    "                    x_max = int(x_max * (new_size[0] / img.width))\n",
    "                    y_max = int(y_max * (new_size[1] / img.height))\n",
    "\n",
    "                x_min, y_min, x_max, y_max = checkBB(x_min, y_min, x_max, y_max)\n",
    "                \n",
    "                new_parts = [parts[0], str(x_min), str(y_min), str(x_max), str(y_max)]\n",
    "                new_label_data.append(\" \".join(new_parts))\n",
    "            \n",
    "                output_label_filename = os.path.splitext(new_file_name + str(sample_n))[0] + \".txt\"\n",
    "                output_label_path = os.path.join(label_final_dir, output_label_filename)\n",
    "            with open(output_label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(new_label_data))\n",
    "\n",
    "            sample_n += 1\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the main folder into train/validate/test folders randomly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4231\n",
      "4231\n",
      "done saving train/valid/test folders!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "image_final_dir = \"Datasets/Concatenated_Dataset/images/\"\n",
    "os.makedirs(image_final_dir, exist_ok = True)\n",
    "\n",
    "label_final_dir = \"Datasets/Concatenated_Dataset/annotations/\"\n",
    "\n",
    "\n",
    "print(len(os.listdir(image_final_dir)))\n",
    "print(len(os.listdir(label_final_dir)))\n",
    "\n",
    "#files = [os.path.splitext(imgfile)[0] for imgfile in os.listdir(image_final_dir)]\n",
    "#files2 = [os.path.splitext(imgfile)[0] for imgfile in os.listdir(label_final_dir)]\n",
    "\n",
    "image_files = os.listdir(image_final_dir)\n",
    "label_files = os.listdir(label_final_dir)\n",
    "\n",
    "# 20% testing set, 20% validation, 60% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_files, label_files, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "image_splits = [X_train, X_val, X_test]\n",
    "label_splits = [y_train, y_val, y_test]\n",
    "folders = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "for idx, set in enumerate(folders):\n",
    "\n",
    "    dir = \"Data/\" + set\n",
    "    img_dir = dir + \"/images\"\n",
    "    label_dir = dir + \"/labels\"\n",
    "\n",
    "    os.makedirs(img_dir, exist_ok = True)\n",
    "    os.makedirs(label_dir, exist_ok = True)\n",
    "\n",
    "    images = image_splits[idx]\n",
    "    labels = label_splits[idx]\n",
    "    for img_file in images:\n",
    "        original_im_dir = os.path.join(image_final_dir, img_file)\n",
    "        target_im_dir = os.path.join(img_dir, img_file)\n",
    "\n",
    "        shutil.copy(original_im_dir, target_im_dir)\n",
    "\n",
    "    for label_file in labels:\n",
    "        original_lab_dir = os.path.join(label_final_dir, label_file)\n",
    "        target_lab_dir = os.path.join(label_dir, label_file)\n",
    "\n",
    "        shutil.copy(original_lab_dir, target_lab_dir)\n",
    "\n",
    "print(\"done saving train/valid/test folders!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(origin, point, angle):\n",
    "    ox, oy = origin # center original image\n",
    "    px, py = point # bounding box corners\n",
    "\n",
    "    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)\n",
    "    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)\n",
    "    return int(qx), int(qy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import imgaug.augmenters as iaa\n",
    "import cv2\n",
    "\n",
    "def rotate(origin, point, angle):\n",
    "    ox, oy = origin # center original image\n",
    "    px, py = point # bounding box corners\n",
    "\n",
    "    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)\n",
    "    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)\n",
    "    return int(qx), int(qy)\n",
    "\n",
    "\n",
    "def data_augmentation(img_dir, img_name, label_path, img_final_dir, label_final_dir, sample_n = 0):\n",
    "    image_original = cv2.imread(os.path.join(img_dir, img_name))\n",
    "    augmented_file_name = \"augm_sample_\"\n",
    "\n",
    "    hflip= iaa.Fliplr(p=1.0) # p: probability of being transformed\n",
    "    #vertical flip\n",
    "    vflip= iaa.Flipud(p=1.0) # p: probability of being transformed\n",
    "    #image rotation\n",
    "    angle = random.randrange(-90,90) # can be changed\n",
    "    rot1 = iaa.Affine(rotate=(angle, angle))\n",
    "    #noise\n",
    "    noise = iaa.AdditiveGaussianNoise(random.randrange(-50,0),random.randrange(50))\n",
    "    #hue and saturation\n",
    "    hue_saturation = iaa.AddToHueAndSaturation((-25, 25), per_channel=True)\n",
    "    #brightness\n",
    "    brightness = iaa.MultiplyAndAddToBrightness(mul=(0.8, 1.2), add=(-30, 30))\n",
    "\n",
    "    #crop \n",
    "    # Did not finish these labels transformation\n",
    "    cropped_percent = random.uniform(0.1, 0.45)\n",
    "    crop1 = iaa.Crop(percent=cropped_percent) # fraction removed from all sides of the image \n",
    "                                              # E.g. if this is set to 0.1, the augmenter will always crop 10% of the imageâs\n",
    "                                              # height at both the top and the bottom (both 10% each), as well as 10% of the width at the right and left\n",
    "    \n",
    "    #shear (already a random amount)\n",
    "    #shear = iaa.Affine(shear=(-20,20)\n",
    "\n",
    "    func_labels = [\"hflip\", \"vflip\", \"rotation\", \"noise\", \"hue_saturation\", \"brightness\"]\n",
    "    funcs=[hflip, vflip, rot1, noise, hue_saturation, brightness]\n",
    "\n",
    "    \n",
    "    # randmly sample some number of transformations to do\n",
    "    num_of_transforms = random.randint(1, len(funcs)-1)\n",
    "    if num_of_transforms == 1:\n",
    "        randInt = random.randint(0, len(funcs)-1)\n",
    "        random_func_labels = [func_labels[randInt]]\n",
    "        random_funcs = [funcs[randInt]]\n",
    "    else:\n",
    "        random_func_labels, random_funcs = zip(*random.sample(list(zip(func_labels, funcs)), num_of_transforms))\n",
    "    #print(random_func_labels)\n",
    "\n",
    "    # TRANSFORM THE IMAGES\n",
    "    new_img = image_original\n",
    "    for function in random_funcs:\n",
    "        new_img = function.augment_image(new_img)\n",
    "    \n",
    "    augmented_image = Image.fromarray(new_img)\n",
    "    augmented_image.save(os.path.join(img_final_dir, augmented_file_name + str(sample_n)+\".jpg\"))\n",
    "    \n",
    "    #TRANSFORM THE BOUNDING BOXES\n",
    "    non_BB_transforms = [\"noise\", \"hue_saturation\", \"brightness\"] # these are the transformations that don't require BB transforms\n",
    "    if all(item in random_func_labels for item in non_BB_transforms):  # check that all transforms are of these types, otherwise we have work to do\n",
    "        target_lab_dir = os.path.join(label_final_dir, augmented_file_name + str(sample_n)+\".txt\")\n",
    "        shutil.copy(label_path, target_lab_dir)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = f.readlines()\n",
    "        # Resize the bounding boxes in the label fileÃ§\n",
    "        new_label_data = []\n",
    "        for line in label_data:\n",
    "            parts = line.strip().split(\" \")\n",
    "            x_min = float(parts[1])\n",
    "            y_min = float(parts[2])\n",
    "            x_max = float(parts[3])\n",
    "            y_max = float(parts[4])\n",
    "\n",
    "            # do the transformations of the BBs for functions that require it\n",
    "            # loop through them so that the BB transforms are done in the same order as the Img transforms\n",
    "            for funcLabel in [funcLabel for funcLabel in random_func_labels if funcLabel not in non_BB_transforms]:\n",
    "\n",
    "                # Convert bounding boxes after transformation\n",
    "                if funcLabel == \"hflip\":\n",
    "                    x_min_new = int(augmented_image.width - x_min -1)\n",
    "                    y_min_new = y_min\n",
    "                    x_max_new = int(augmented_image.width - x_max -1)\n",
    "                    y_max_new = y_max\n",
    "                elif funcLabel == \"vflip\":\n",
    "                    x_min_new = x_min\n",
    "                    y_min_new = int(augmented_image.height - y_min -1)\n",
    "                    x_max_new = x_max\n",
    "                    y_max_new = int(augmented_image.height - y_max -1)\n",
    "                elif funcLabel == \"rotation\":\n",
    "                    cx = int(np.shape(image_original)[0]/ 2)\n",
    "                    cy = int(np.shape(image_original)[1] / 2)\n",
    "                    bottom_left = rotate([cx, cy], [x_min, y_min], np.radians(angle))\n",
    "                    top_right = rotate([cx, cy], [x_max, y_max], np.radians(angle))\n",
    "                    x_min_new = bottom_left[0]\n",
    "                    y_min_new = bottom_left[1]\n",
    "                    x_max_new = top_right[0]\n",
    "                    y_max_new = top_right[1]\n",
    "                else:\n",
    "                    print(\"HELLO\")\n",
    "\n",
    "                x_min = x_min_new\n",
    "                y_min = y_min_new\n",
    "                x_max = x_max_new\n",
    "                y_max = y_max_new\n",
    "\n",
    "                x_min, y_min, x_max, y_max = checkBB(x_min, y_min, x_max, y_max)\n",
    "        \n",
    "            new_parts = [parts[0], str(x_min), str(y_min), str(x_max), str(y_max)]\n",
    "            new_label_data.append(\" \".join(new_parts))\n",
    "            output_label_filename = os.path.splitext(augmented_file_name + str(sample_n))[0] + \".txt\"\n",
    "            output_label_path = os.path.join(label_final_dir, output_label_filename)\n",
    "\n",
    "        with open(output_label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(new_label_data))\n",
    "\n",
    "\n",
    "        sample_n += 1\n",
    "    return sample_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = \"Data/train\" \n",
    "img_dir = dir + \"/images\"\n",
    "label_dir = dir + \"/labels\"\n",
    "sample_n = 0\n",
    "#img_final_dir\n",
    "#label_final_dir\n",
    "\n",
    "\n",
    "for filename in os.listdir(img_dir):\n",
    "    # Open the corresponding label file\n",
    "    label_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    label_path = os.path.join(label_dir, label_filename)\n",
    "    for i in range(3): # do 3 random transform sets (so 1 original + 3 new, per original img)\n",
    "        sample_n = data_augmentation(img_dir, filename, label_path, img_dir, label_dir, sample_n) \n",
    "   \n",
    "    #img_dir, img_name, label_path, img_final_dir, label_final_dir, sample_n = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.dataLoading import playersDataset, collate_fn\n",
    "import torch\n",
    "\n",
    "# set the device (GPU is much faster)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_workers = 1 if torch.cuda.is_available() else 0\n",
    "batch_size = 32 # LOWER THIS IF NEEDED!\n",
    "\n",
    "train_dir = \"Data/train/\"\n",
    "valid_dir = \"Data/valid/\"\n",
    "test_dir = \"Data/valid/\"\n",
    "train_dataset = playersDataset(train_dir)\n",
    "validation_dataset = playersDataset(valid_dir)\n",
    "test_dataset = playersDataset(test_dir)\n",
    "\n",
    "# Define the dataloaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
