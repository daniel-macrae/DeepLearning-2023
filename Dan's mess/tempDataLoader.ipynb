{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "##import matplotlib.pyplot as plt\n",
    "##import matplotlib.patches as patches\n",
    "\n",
    "import sys\n",
    "\n",
    "class ImageFolder(Dataset):\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "class playersDataset(Dataset):\n",
    "    def __init__(self, folder_path, img_size=416):\n",
    "        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n",
    "        self.img_shape = (img_size, img_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index % len(self.files)]\n",
    "        # Extract image\n",
    "        img = np.array(Image.open(img_path))\n",
    "        h, w, _ = img.shape\n",
    "        dim_diff = np.abs(h - w)\n",
    "        # Upper (left) and lower (right) padding\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        input_img = np.pad(img, pad, 'constant', constant_values=127.5) / 255.\n",
    "        # Resize and normalize\n",
    "        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n",
    "        # Channels-first\n",
    "        input_img = np.transpose(input_img, (2, 0, 1))\n",
    "        # As pytorch tensor\n",
    "        input_img = torch.from_numpy(input_img).float()\n",
    "\n",
    "        return img_path, input_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, list_path, img_size=416):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.img_files = file.readlines()\n",
    "        self.label_files = [path.replace('images', 'labels').replace('.jpg', '.txt') for path in self.img_files]\n",
    "        self.img_shape = (img_size, img_size)\n",
    "        self.max_objects = 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #---------\n",
    "        #  Image\n",
    "        #---------\n",
    "\n",
    "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "        img = np.array(Image.open(img_path))\n",
    "\n",
    "        # Handles images with less than three channels\n",
    "        while len(img.shape) != 3:\n",
    "            index += 1\n",
    "            img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "            img = np.array(Image.open(img_path))\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        dim_diff = np.abs(h - w)\n",
    "        # Upper (left) and lower (right) padding\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.\n",
    "        padded_h, padded_w, _ = input_img.shape\n",
    "        # Resize and normalize\n",
    "        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n",
    "        # Channels-first\n",
    "        input_img = np.transpose(input_img, (2, 0, 1))\n",
    "        # As pytorch tensor\n",
    "        input_img = torch.from_numpy(input_img).float()\n",
    "\n",
    "        #---------\n",
    "        #  Label\n",
    "        #---------\n",
    "\n",
    "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "        labels = None\n",
    "        if os.path.exists(label_path):\n",
    "            labels = np.loadtxt(label_path).reshape(-1, 5)\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = w * (labels[:, 1] - labels[:, 3]/2)\n",
    "            y1 = h * (labels[:, 2] - labels[:, 4]/2)\n",
    "            x2 = w * (labels[:, 1] + labels[:, 3]/2)\n",
    "            y2 = h * (labels[:, 2] + labels[:, 4]/2)\n",
    "            # Adjust for added padding\n",
    "            x1 += pad[1][0]\n",
    "            y1 += pad[0][0]\n",
    "            x2 += pad[1][0]\n",
    "            y2 += pad[0][0]\n",
    "            # Calculate ratios from coordinates\n",
    "            labels[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "            labels[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "            labels[:, 3] *= w / padded_w\n",
    "            labels[:, 4] *= h / padded_h\n",
    "        # Fill matrix\n",
    "        filled_labels = np.zeros((self.max_objects, 5))\n",
    "        if labels is not None:\n",
    "            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n",
    "        filled_labels = torch.from_numpy(filled_labels)\n",
    "\n",
    "        return img_path, input_img, filled_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
